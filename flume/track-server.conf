k2h.sources = s1  #配置flume数据来源
k2h.channels = c1
k2h.sinks = k1


k2h.sources.s1.type = org.apache.flume.source.kafka.KafkaSource
k2h.sources.s1.channels = c1
k2h.sources.s1.zookeeperConnect =   zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181/kafka
k2h.sources.s1.topic =  maxevent #topic名称
k2h.sources.s1.groupId = flume
k2h.sources.s1.kafka.consumer.timeout.ms = 100


k2h.sources.s1.interceptors.json_interceptor.type =com.maxent.flume.plugins.interceptors.JsonInterceptor$Builder
k2h.sources.s1.interceptors.json_interceptor.logtype = maxevent
k2h.sources.s1.interceptors.json_interceptor.version = REL_1.7.1
k2h.sources.s1.interceptors.json_interceptor.headers = timestamp
k2h.sources.s1.interceptors.json_interceptor.paths = $.timestamp #kafka数据源中有timestamp的话用数据源里的timestamp

k2h.channels.c1.type =  file
k2h.channels.c1.capacity = 10000
k2h.channels.c1.transactionCapacity = 1000
k2h.channels.c1.checkpointDir = ./checkpoint/maxevent
k2h.channels.c1.dataDirs =./flume/data/maxevent_original  #配置hdfs中对应的路径



k2h.channels = c1
k2h.sinks.k1.type = hdfs
k2h.sinks.k1.channel = c1
k2h.sinks.k1.hdfs.path = /flume/maxevent_original/day=%Y%m%d/hour=%H  #配置hdfs中对应的路径
k2h.sinks.k1.hdfs.filePrefix = events-
k2h.sinks.k1.hdfs.inUsePrefix = .
k2h.sinks.k1.hdfs.round = true
k2h.sinks.k1.hdfs.roundValue = 10
k2h.sinks.k1.hdfs.roundUnit = minute
k2h.sinks.k1.hdfs.fileType = DataStream
k2h.sinks.k1.hdfs.rollSize = 0
k2h.sinks.k1.hdfs.rollCount = 0
k2h.sinks.k1.hdfs.rollInterval = 300
k2h.sinks.k1.hdfs.timeZone =  UTC